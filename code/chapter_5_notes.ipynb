{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Part 2: Building and Training a Neural Network From Sratch \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak inside the MNIST data as delivered from the keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def shape_data(data_images, data_labels):\n",
    "    features = [np.reshape(x, (784, 1)) / 255.0 for x in data_images]\n",
    "    labels = [to_categorical(y, num_classes=10) for y in data_labels]\n",
    "    return list(zip(features, labels))\n",
    "\n",
    "def load_data():\n",
    "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "    shaped_train_data = shape_data(train_images, train_labels)\n",
    "    shaped_test_data = shape_data(test_images, test_labels)\n",
    "    return shaped_train_data, shaped_test_data\n",
    "\n",
    "shaped_train_data, shaped_test_data = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(shaped_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x is a two-dimensional array with 784 rows and 1 column. this is essentially a column vector with 784 elements\n",
    "shaped_train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaped_train_data[0][0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground truth y is a 1D array containing 10 elements\n",
    "\n",
    "shaped_train_data[0][1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaped_test_data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network From Scratch \n",
    "#### *from the book Deep Learning and the Game of Go, with some minor modifications*\n",
    "\n",
    "### Changes\n",
    "- Minor linting support added \n",
    "- Deltas are processed in column vectors in np arrays instead of lists\n",
    "- Added printouts of progressive accuracy over the epochs\n",
    "- Using xavior initialization instead of random initialization of weights to handle dissapearing gradients\n",
    "- Reduced learning rate from 3 to .9 to handle exploding gradients (or what I supposed to be exploding gradients and which was resulting in a failing model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network...\n",
      "|--DenseLayer\n",
      "    |-- dimensions: (784, 392)\n",
      "|-- ActivationLayer\n",
      " |-- dimensions: (392,392)\n",
      "|--DenseLayer\n",
      "    |-- dimensions: (392, 196)\n",
      "|-- ActivationLayer\n",
      " |-- dimensions: (196,196)\n",
      "|--DenseLayer\n",
      "    |-- dimensions: (196, 10)\n",
      "|-- ActivationLayer\n",
      " |-- dimensions: (10,10)\n",
      " initial sum of weights of the last layer: -2.6337755720653\n",
      "Epoch 0 complete. Average Training loss: 0.7469072502517705\n",
      "Sum of weights of the last layer after epoch 0: -258.349462177276\n",
      "Epoch 0: 9244 / 10000\n",
      "Epoch 1 complete. Average Training loss: 0.8331697449158939\n",
      "Sum of weights of the last layer after epoch 1: -296.1944352784897\n",
      "Epoch 1: 9438 / 10000\n",
      "Epoch 2 complete. Average Training loss: 0.8461794080545909\n",
      "Sum of weights of the last layer after epoch 2: -309.34416223099527\n",
      "Epoch 2: 9575 / 10000\n",
      "Epoch 3 complete. Average Training loss: 0.8547710953048605\n",
      "Sum of weights of the last layer after epoch 3: -326.8422937277082\n",
      "Epoch 3: 9625 / 10000\n",
      "Epoch 4 complete. Average Training loss: 0.8606661363392539\n",
      "Sum of weights of the last layer after epoch 4: -327.94837660577423\n",
      "Epoch 4: 9663 / 10000\n",
      "Epoch 5 complete. Average Training loss: 0.8648717427621709\n",
      "Sum of weights of the last layer after epoch 5: -337.17292993838305\n",
      "Epoch 5: 9686 / 10000\n",
      "Epoch 6 complete. Average Training loss: 0.8681187450560895\n",
      "Sum of weights of the last layer after epoch 6: -347.08405960406384\n",
      "Epoch 6: 9733 / 10000\n",
      "Epoch 7 complete. Average Training loss: 0.8711544442005831\n",
      "Sum of weights of the last layer after epoch 7: -354.6521913375805\n",
      "Epoch 7: 9729 / 10000\n",
      "Epoch 8 complete. Average Training loss: 0.8734454755782722\n",
      "Sum of weights of the last layer after epoch 8: -356.7304091730656\n",
      "Epoch 8: 9747 / 10000\n",
      "Epoch 9 complete. Average Training loss: 0.8754651931667328\n",
      "Sum of weights of the last layer after epoch 9: -362.65311905023884\n",
      "Epoch 9: 9761 / 10000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def sigmoid_double(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return np.vectorize(sigmoid_double)(z)\n",
    "\n",
    "\n",
    "def sigmoid_prime_double(x):\n",
    "    return sigmoid_double(x) * (1 - sigmoid_double(x))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return np.vectorize(sigmoid_prime_double)(z)\n",
    "\n",
    "\n",
    "class MSE:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_function(predictions, labels):\n",
    "        diff = predictions - labels\n",
    "        return 0.5 * sum(diff * diff)[0] \n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_derivative(predictions, labels):\n",
    "        predictions = predictions.flatten()\n",
    "        labels = labels.flatten()\n",
    "        return predictions - labels\n",
    "\n",
    "    \n",
    "\n",
    "class Layer:  \n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.previous = None  \n",
    "        self.next = None  \n",
    "\n",
    "        self.input_data = None  \n",
    "        self.output_data = None\n",
    "\n",
    "        self.input_delta = None  \n",
    "        self.output_delta = None\n",
    "\n",
    "    def connect(self, layer):\n",
    "\n",
    "        self.previous = layer\n",
    "        layer.next = self\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_forward_input(self):\n",
    "        if self.previous is not None:\n",
    "            return self.previous.output_data\n",
    "        else:\n",
    "            return self.input_data\n",
    "\n",
    "    def backward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_backward_input(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "\n",
    "        output shape is column vector (n, 1) due to processing needs in DenseLayer.backward()\n",
    "        \"\"\"\n",
    "        if self.next is not None:\n",
    "            backward_feed_delta = self.next.output_delta\n",
    "        else:\n",
    "            backward_feed_delta = self.input_delta\n",
    "        \n",
    "        backward_feed_delta = backward_feed_delta.reshape(-1, 1)\n",
    "        return backward_feed_delta\n",
    "        \n",
    "    def clear_deltas(self):\n",
    "        pass\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        pass\n",
    "    \n",
    "    def describe(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ActivationLayer, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = input_dim \n",
    "            \n",
    "    def forward(self):\n",
    "        data = self.get_forward_input()\n",
    "        self.output_data = sigmoid(data)\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        delta = self.get_backward_input()\n",
    "        data = self.get_forward_input()\n",
    "\n",
    "        self.output_delta = delta * sigmoid_prime(data)\n",
    "\n",
    "    def describe(self):\n",
    "        print(\"|-- \" + self.__class__.__name__)\n",
    "        print(\" |-- dimensions: ({},{})\".format(self.input_dim, self.output_dim))\n",
    "\n",
    "\n",
    "class DenseLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim, initialization=\"random\"):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        if initialization == \"xavier\":\n",
    "            a = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "            self.weight = np.random.uniform(-a, a, (output_dim, input_dim))\n",
    "        else:\n",
    "            self.weight = np.random.randn(output_dim, input_dim)\n",
    "\n",
    "        self.bias = np.random.randn(output_dim, 1)\n",
    "\n",
    "        self.params = [self.weight, self.bias]\n",
    "\n",
    "        self.delta_w = np.zeros(self.weight.shape)\n",
    "        self.delta_b = np.zeros(self.bias.shape)\n",
    "\n",
    "    def forward(self):\n",
    "        data = self.get_forward_input()\n",
    "        self.output_data = np.dot(self.weight, data) + self.bias\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        data = self.get_forward_input()\n",
    "        delta = self.get_backward_input()\n",
    "\n",
    "        self.delta_b += delta\n",
    "        self.delta_w += np.dot(delta, data.transpose())\n",
    "        self.output_delta = np.dot(self.weight.transpose(), delta)\n",
    "  \n",
    "    def update_params(self, learning_rate):\n",
    "\n",
    "        self.weight -= learning_rate * self.delta_w\n",
    "        self.bias -= learning_rate * self.delta_b\n",
    "\n",
    "    def clear_deltas(self):\n",
    "        self.delta_w = np.zeros(self.weight.shape)\n",
    "        self.delta_b = np.zeros(self.bias.shape)\n",
    "\n",
    "    def describe(self):\n",
    "        print(\"|--\" + self.__class__.__name__)\n",
    "        print(\"    |-- dimensions: ({}, {})\".format(self.input_dim, self.output_dim))\n",
    "\n",
    "\n",
    "class SequentialNetwork:\n",
    "    def __init__(self,loss=None):\n",
    "        print(\"Initializing network...\")\n",
    "        self.layers = []\n",
    "        if loss is None:\n",
    "            self.loss = MSE()\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        layer.describe()\n",
    "        if len(self.layers) > 1:\n",
    "            self.layers[-1].connect(self.layers[-2])\n",
    "\n",
    "    def train(self, training_data, epochs, mini_batch_size, learning_rate, test_data=None):\n",
    "        \"\"\"\n",
    "        This function differs from the one in the book because it reporst on progress for each epoch and tests on the test data after each epoch\n",
    "        \"\"\"\n",
    "        n = len(training_data)\n",
    "\n",
    "        initial_weights_sum = np.sum(self.layers[-2].weight)\n",
    "        print(f\" initial sum of weights of the last layer: {initial_weights_sum}\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k + mini_batch_size] for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "\n",
    "            epoch_loss = 0 \n",
    "\n",
    "            for mini_batch in mini_batches:\n",
    "                batch_loss = self.train_batch(mini_batch, learning_rate)  \n",
    "                epoch_loss += batch_loss  \n",
    "\n",
    "            epoch_loss /= len(mini_batches)   \n",
    "\n",
    "            print(f\"Epoch {epoch} complete. Average Training loss: {epoch_loss}\")\n",
    "\n",
    "            epoch_weights_sum = np.sum(self.layers[-2].weight)\n",
    "            print(f\"Sum of weights of the last layer after epoch {epoch}: {epoch_weights_sum}\")\n",
    "\n",
    "            if test_data:\n",
    "                n_test = len(test_data)\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(epoch, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(epoch))\n",
    "\n",
    "    def train_batch(self, mini_batch, learning_rate):\n",
    "        \n",
    "        self.forward_backward(mini_batch)\n",
    "\n",
    "        batch_loss = 0  \n",
    "\n",
    "        for x, y in mini_batch:\n",
    "            batch_loss += self.loss.loss_function(self.single_forward(x), y)\n",
    "\n",
    "        batch_loss /= len(mini_batch)  \n",
    "\n",
    "\n",
    "        self.update(mini_batch, learning_rate)\n",
    "\n",
    "        return batch_loss \n",
    "    \n",
    "    def update(self, mini_batch, learning_rate):\n",
    "        learning_rate = learning_rate / len(mini_batch)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.update_params(learning_rate)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.clear_deltas()\n",
    "\n",
    "    def forward_backward(self, mini_batch):\n",
    "        for x, y in mini_batch:\n",
    "            self.layers[0].input_data = x  # seting here the input data for the first layer directly\n",
    "            \n",
    "            # start up forward propagation\n",
    "            for layer in self.layers:\n",
    "                layer.forward()\n",
    "\n",
    "            self.layers[-1].input_delta = self.loss.loss_derivative(self.layers[-1].output_data, y)\n",
    "\n",
    "            # ...backward propagation\n",
    "            for layer in reversed(self.layers):\n",
    "                layer.backward()\n",
    "\n",
    "    def single_forward(self, x):\n",
    "        \"\"\"\n",
    "        used for testing \n",
    "        \"\"\"\n",
    "        self.layers[0].input_data = x  # set input data for the first layer same as in forward_backward(). \n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.forward()\n",
    "            \n",
    "        return self.layers[-1].output_data\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(\n",
    "            np.argmax(self.single_forward(x)),\n",
    "            np.argmax(y)\n",
    "        ) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "    \n",
    "net2 = SequentialNetwork()\n",
    "net2.add(DenseLayer(784, 392, initialization=\"xavier\"))\n",
    "net2.add(ActivationLayer(392))\n",
    "net2.add(DenseLayer(392, 196, initialization=\"xavier\"))\n",
    "net2.add(ActivationLayer(196))\n",
    "net2.add(DenseLayer(196, 10, initialization=\"xavier\"))\n",
    "net2.add(ActivationLayer(10))\n",
    "\n",
    "shaped_train_data, shaped_test_data = load_data()\n",
    "\n",
    "\n",
    "# shaped_train_data = shaped_train_data[:10000]\n",
    "# shaped_test_data = shaped_test_data[:10000]\n",
    "\n",
    "# shaped_train_data, shaped_test_data \n",
    "net2.train(shaped_train_data, epochs=10, mini_batch_size=10,\n",
    "          learning_rate=.9, test_data=shaped_test_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label counts: Counter({1: 6742, 7: 6265, 3: 6131, 2: 5958, 9: 5949, 0: 5923, 6: 5918, 8: 5851, 4: 5842, 5: 5421})\n",
      "Test label counts: Counter({1: 1135, 2: 1032, 7: 1028, 3: 1010, 9: 1009, 4: 982, 0: 980, 8: 974, 6: 958, 5: 892})\n"
     ]
    }
   ],
   "source": [
    "# a bit more info on the data \n",
    "\n",
    "train_labels_int = [np.argmax(label) for _, label in shaped_train_data]\n",
    "test_labels_int = [np.argmax(label) for _, label in shaped_test_data]\n",
    "from collections import Counter\n",
    "\n",
    "train_label_counts = Counter(train_labels_int)\n",
    "test_label_counts = Counter(test_labels_int)\n",
    "\n",
    "print(\"Train label counts:\", train_label_counts)\n",
    "print(\"Test label counts:\", test_label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
